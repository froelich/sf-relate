# ================== MACHINE CONFIGURATIONS ==================
# ============= IS AT THE END OF THIS FILE ===================

# ================== EXPERIMENT CONFIGURATIONS ==================
## ==================== STEP 0 (BASIC) ===========================
N = 204928 # size of hash tables (recommend: 64 * total number of individuals on both parties)
# Increase N to improve recall, 
# at the cost of inicreasing number of comparisons and thus runtime in step 2

## ==================== STEP 0 (ADVANCED) ========================
# The following advanced parameters come with default values. 
# While the default values should work well on most datasets, 
# users can modify them based on their needs, notably when specific IBD structures are known about the datasets.
enclen = 80 # the number of snps in each encoded split haplotype segment
seglen = 8.0 # centi-Morgan length of each split haplotype segment
steplen = 4.0 # centi-Morgan spacing between the beginning of each split haplotype segment
k = 8 # number of SNPs in each kSNP token for hashing
l = 4 # number of hash tokens to construct every hash index
maxL = 6 # max number of repetitive hashing; increase and retry if table saturation is low (should be larger than the argument to step_1_hashing.py)
s = 0.7 # subsampling rate (i.e. num(outputSNPs)/num(SNPs in .pvar))

## ======================== STEP 1 ===============================
L = 3 # number of repetitive hashing; increase and retry if table saturation is low; if not enough repetition hash keys are sampled in step0 (default number of keys is maxL=6), redo step0 with a larger maxL.
## ======================== STEP 2 ===============================
PARA = 3 # Number of parallel processes to use. Set to 20 for the UKB dataset with 100K individual * 90K SNPs on the Google Cloud machine with 128 cores and 576GB memory. Should be set as large as possible to utilize all CPUs and memory. Exact value depends on the machine and dataset sizes. Users can provide reasonable parameters like 5 and retry with a smaller one if it fails due to memory constraints.

# select output modes
reveal = 0
# reveal = 1
# reveal = 2
# reveal = 3
# reveal = 0 is SF-Related's default (one indicator per individual per party) that computes whether the max kinship is larger than the degree 3 threshold
# reveal = 1 computes 4 indicators per individual per party, each meaning whether the max kinship is larger than the corresponding degree threshold in thres_value 
thresh_value= [1.8232, 1.6464, 1.2928, 0.5857] # correspond to degree 3, 2, 1, 0
# reveal = 2 computes 16 indicators per individual per party using the following thresholds (can be customized, but the runtime of phase 2 will scale accordingly.)
discretized_thresh= [2.0,1.9375,1.875,1.8125,1.75,1.6875,1.625,1.5625,1.5,1.375,1.25,1.125,1.0,0.75,0.5,0.25] # each sub-interval between the degrees are is split into 4
# reveal = 3 reveals all computed intermediate kinship and decrypt them.

# ============ ADVANCED/INTERNAL PROTOCOL CONFIGURATIONS ============
# ================== DO NOT MODIFY BELOW THIS LINE ==================
# ================= UNLESS YOU UNDERSTAND THE CODE ==================
# Data
npz = true
start_key = 0
total_nbr_rows = 0 # if <= 0 --> read from npz 
total_nbr_rows_test = 0 # test only a subset of the rows # if <= 0 --> all rows
number_of_columns = 0 # if <= 0 --> read from file
# demo experiment that only processes the first 100 SNPs
number_of_columns_test = 0 # test only a subset of the columns # if <= 0 --> all columns

batch_length = 8192 # for large-scale experiments, should be 8192 for HE
block_limit = 100 
# approx for HE only
degree = 30
iter = 3
A = 0.0
B = 50.0
scale_down = 0.0 # for HE only, need to be increased with the data dimension (nbr of SNPs, #het)
scale_down_discretize = [ 2.0, 2.0, 2.0, 2.0, 1.0] 
debug_st_flag = false # if true, will save the intermediate results of the protocol

Blinding = false # fixed, old feature, might be reintroduced at some point
single = false # fixed, old feature, might be reintroduced at some point
local_test = false # should be false 
comparison_map = {"1" = [2]} # fixed for 2 parties experiments
boot_map = {"1" = 1, "1_boot" = 3, "1_dec" = 5, "2" = 2, "2_boot" = 4, "2_dec" = 6} # can be left like this for 2 parties

## Crypto parameters
# Options: PN12QP109, PN13QP218, PN14QP438, PN15QP880, PN16QP1761
# Defined in ckks/params.go in Lattigo library
ckks_params = "PN14QP438" # fixed

num_main_parties = 2
hub_party_id = 1
debug = false # should be false for timing experiments
test_sign_test = 0 # 0 means the main experiment

# MPC parameters
div_sqrt_max_len = 100000 # fixed
# num channels --- maybe need to reduce if num of cores is fewer
mpc_num_threads = 400 # communication channels. HE: should be at least PARA*((NumMainParties*3)+1), MPC: should be at least PARA

mpc_field_size = 256 # fixed
mpc_data_bits = 60 # fixed
mpc_frac_bits = 30 # fixed
mpc_boolean_shares = true # fixed
use_mpc = false # if use MPC true, for HE --> false
# deprecated
bucket_size	= 1
# ================== DO NOT MODIFY ABOVE THIS LINE ==================

# ================== MACHINE CONFIGURATIONS ==================
# Shared keys used in the protocols
shared_keys_path = "config/demo/keys/"

# num threads --- should be set to about 10 * num of cores
nbr_threads = 1280 

# Ports for listening to incoming connections from the other parties
# Party 0 & 1 are hosted on the same machine
[servers.party0]
ipaddr = "127.0.0.1" #should be local IP of GCP machine
ports  = {party1 = "5110", party2 = "7320"}

[servers.party1]
ipaddr = "127.0.0.1"
ports  = {party2 = "9210"}

[servers.party2]
ipaddr = "127.0.0.1"
ports  = {}
# ============================ END OF FILE ==========================

# ================== DO NOT MODIFY BELOW THIS LINE ==================
